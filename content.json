{"pages":[{"title":"About me","text":"About Me1234567def main(): '''this is a welcome function :)''' print \"Say hello!\" print 'Hi! Welcome to Muyoo and hope you will enjoy here!'if __name__ == '__main__': main()看到上面这行代码的朋友应该就明白了：我的职业是攻城狮（绝对不承认是程序猿）。 俗话说得好，“好笔头不如烂笔杆”，所以我觉得自己有任何想法想要让自己后来还记得的话，最终还是得落到纸上。然而现在网络科技这么便捷，在网上用键盘写文章自然就代替纸笔了，还能分享给别人看，何乐不为？（嘛，虽然伦家字迹也很好看的说~） 这里的每篇文章都是随性而写，有关IT技术的，有关生活点滴的，有关旅行梦想的……我都可以保证它们均出自我手。而且，每篇文章的封面图也同样是我的创作哟~ 朋友们来到这里，也是一种缘分，如若还能在这小憩之地得到一点慰藉或愉悦，那便更是好上加好的小缘了~ 潇慕雨Muyoo Contact Me: xmuyoo@qq.comWeChat: xmuyoo","link":"/about/index.html"}],"posts":[{"title":"Kafka 配置项","text":"Kafka 部分配置项本文只记录一些目前使用Kafka可能会比较有用的配置项。关于单个broker的，和关于topic-level的。其中有些配置项可以是topic级别的。Kafka的版本是0.8.2.1。 log.flush.interval：默认500，数据积累的量，超过这个量就将数据写到磁盘 log.default.flush.interval.ms：数据写到磁盘的间隔时间，毫秒。默认与log.default.flush.scheduler.interval.ms相同，3000。 log.retention.hours：数据保留的时间，小时。默认168 log.segment.bytes：数据文件大小，默认是102410241024 log.roll.{ms,hours}：数据文件切分的时间间隔，与大小限制同时起作用的，默认是24*7小时 log.cleanup.policy：处理切分后数据的方式，delete或者compact。如果是delete，被切分出来的数据会被删除；如果是compact，切分出的数据会被压缩用于淘汰过期的数据。 log.cleaner.min.cleanable.ratio： min.insync.replicas： log.roll.jitter.{ms,hours}： Topic-Level topic.flush.intervals.ms：覆盖log.default.flush.interval.ms。 topic.log.retention.hours：覆盖log.retention.hours topic.partition.count.map：覆盖创建topic时指定的partition数量。 topic.log.segment.bytes：覆盖log.segment.bytes topic.log.roll.{ms,hours}：覆盖log.roll.{ms,hours} topic.log.cleanup.policy：覆盖log.cleanup.policy min.cleanable.dirty.ratio：覆盖log.cleaner.min.cleanable.ratio min.insync.replicas：覆盖min.insync.replicas segment.jitter.ms：覆盖log.roll.jitter.{ms,hours} 官方的topic level配置项：topic level","link":"/2015/10/22/KafkaConfig/"},{"title":"Flink 扯蛋 —— Window","text":"流计算中的 Window首先，我们需要时刻铭记的是：在流计算中，数据永不停歇、永无止境。也就是说，在流计算的体系中数据是无限的。这一点和其它绝大多数计算场景非常不同。 数据无止境，但计算是有边界的。如何让有边界的计算去处理一个无边界的数据？这就在流计算中引入了 Window 的概念。 Window 在流计算中就是指一个数据窗口，Window 定义了一个边界。它将数据流上的部分数据划入边界范围内，目的是得到一个有边界的数据集合。因为 Window 有边界，所以被划到一个 Window 中的数据集合也变成了有边界、有限的。现在，有边界的计算就能够处理这同样有边界的数据集合了。 流计算中，定义 Window 边界的维度只有：时间。一个 Window 的下界是较早的时间点，上界是较晚的时间点。在某一时刻去观察一个 Window 时，它的上下界是确定的，被它的上下界划定的数据集合是确定的。而定义一个 Window 的长度的维度有：时间或元素数量。 Flink 中的 Window在 Flink 中，Window 的概念与上面解释的概念并无区别，只是作为流计算的工程实现结果，Flink 对 Window 在具体表现方式上定义了不同的类型。 Window 的种类Time Tumbling Window 是一种边界相对固定的窗口，其它一些地方也称之为 Fixed Window。这种窗口的特点是：若有多个窗口，它们各自定义的时间范围不会有交集： $$T_i \\bigcap T_j \\in \\varnothing$$ 但两个窗口各自定义的时间边界可能是连在一起的。理论上严格来说，在这种窗口内的数据同样是永无止境的。但由于我们已经为它添加了一个时间边界的约束条件，所以我们通常认为它是类似有限并且可计算的。 例如定义两个 Window：$W_1 = [10:00,11:00)$ 和 $W_2 = [11:00, 12:00)$。所有符合这两个窗口所定义的时间边界的数据都会被分配到对应的窗口中。 Time Sliding Window 是一种边界会随着时间移动而改变的窗口。通常我们直接定义的是这种窗口的时间长度，例如“最近 1 小时”、“最近 5 分钟”、“30 分钟前的 1 小时” 等。多个窗口之间的时间范围，可能是有交集的：$$T_i \\bigcap T_j \\notin \\varnothing$$ 在这种窗口下，一个数据元素很可能被划定到多个窗口中。和 Time Tumbling Window 类似，理论上这种窗口内的数据也是无限的。 例如定义两个窗口：$W_1 = [10:00, 11:00]$ 和 $W_2 = [10:30, 11:30]$，以及数据元素 $e = 10:25$。此时，$e$ 既属于 $W_1$ 也属于 $W_2$。 Count Tumbling Window 是一种边界会随时间变化而推移的窗口，当元素数量达到窗口长度时，窗口固定。我们通常只声明窗口长度。在这种窗口内，数据元素数量是有限的，但时间边界可能永远不会固定下来。多个窗口之间的元素集合没有交集，因为它定义的是“每 N 个元素在一个窗口内”。 例如定义： $W_1, \\space with \\space max(count) = 100$，那么数据流上每 100 个元素为一个窗口；当一个窗口中的元素永远达不到 100 时，这个窗口的上界就会一直推移下去；这个窗口的边界也就不会确定。 Count Sliding Window 同样这种窗口的边界会随时间变化而推移。与 Count Tumbling Window 类似的是，我们通常只声明窗口的长度；而与之不同的是，它的推移方式是以窗口的单位长度进行推移，而不是窗口的整体长度，比如“最近 100 个元素”，“最近 50 个元素”等。多个窗口之间的元素集合可能是有交集的。 数据在 Window 中是怎么流转的在这借助 Flink 官方的示意图： 从一个数据元素进入流开始，WindowAssigner 为这个数据元素分配一个 Window；接着，Trigger 检查这个 Window 的输出条件是否被满足。如果条件达到了，并且也有一个 Evictor，那么 Trigger 将此时 Window 中所有的数据元素交给 Evictor；如果没有 Evictor，那么 Trigger 就直接将这些数据元素交给 EventFunction。Evictor 在收到数据元素后过滤掉不需要的数据元素，然后再将数据交给 EventFunction。EventFunction 收到数据后，就开始对数据元素做计算的工作（例如求和、计数），最后将结果输出。 Trigger 的功能是检测是否满足输出窗口内数据的条件，然后将数据输出；并决定是否继续保留这个窗口。为 Trigger 声明的检测条件是自定义的。 之前介绍的两种 Count Window 理论上其时间边界依然是无限延续的，但通过 Trigger 设置一个最长的等待时间后，就可以使 Count Window 不必永远等待下去。与 Count Window 的思路类似，Time Window 的每个窗口内的元素数量理论上同样是无穷的。但通过 Window Assigner 和 Trigger 的协助，Window Assigner 可以将时间不被接受的数据元素拒之门外，同时 Trigger 也可以通过设置一个窗口内的元素数量上限来使窗口内的元素数据量变为有限的。 Evictor 数据的过滤器，它的功能是在接收到 Trigger 传递的数据元素后，按照声明的过滤条件将数据过滤一遍。 Evaluation Function 它的功能是对数据元素做计算操作，例如求和、计数、求最大值或是其它一般操作。它的计算结果可能是多个的。 这些组件在 Flink 中均可以通过 DataStream API 来使用。 附录扯一下流中的时间在计算机的世界里没有绝对准确的时间。这是因为在我们的现实世界中，绝对时间就不存在。作为三次元的生物，我们只能参考物质变化的表现来间接地感知“时间”这个四维空间中的维度变化。为了直观地记录时间，我们使用了 “时钟”。 当使用不同的时钟时，记录的时间值可能不同（例如我们常说的“钟快了”、“钟慢了”）。在流计算的体系里，这种不同体现在“数据产生的时间”和“在系统内数据被处理时的时间”上。这两种时间通常是不一样的。在计算时，需要选择其中一种来定义 Window 的边界。 备注：所谓的“真正准确的时间”在计算的世界里，通常被称为墙上时钟，而墙上时钟是不存在的。现实世界中使用铯-133 原子基态的两个超精细能级之间跃迁所对应的辐射（电磁波）的 9192631770 个周期持续的时间来定义“1 秒”的单位时间。而这也使用了原子作为参照物。所幸的是，在数据计算的过程中，我们通常不需要知道墙上时钟真正的值是多少；我们关心的通常是一个“边界”和“先后顺序”。 参考 Flink Introducing Windows Flink Windows Second","link":"/2019/10/21/Flink-WindowAndTime/"},{"title":"Kafka 水平扩展、集群镜像","text":"之前阅读了Kafka的基本概念和关于Replica的文档，现在已是晕晕乎乎。在这大好的清晨微风徐徐中，继续阅读关于Kafka集群镜像的内容。（Kafka版本：0.8.2.1） Kafka集群镜像Kafka的集群镜像与Replica不同。Replica作用在单个集群中不同的节点之间；而建立集群镜像在不同的Kafka集群之间发生作用。在集群镜像的过程中，源数据来自于一个或多个不同的Kafka集群，然后数据流向一个目标集群：Kafka提供了专用的工具实现创建集群镜像。 这个操作的一个用途之一是创建数据中心的备份。但在严格意义上讲，它不完全是“备份”。因为创建后的镜像与源集群在partition数量、offset方面都会有所不同。但由于其使用的message key仍然不变并且也是通过Consumer的方式读取数据的，所以其顺序仍然相同。 扩展Kafka集群添加新broker：为新的broker分配一个唯一id，然后在新的服务器上启动Kafka即可。做到这一步完成新broker的添加，但它只有在创建新的topic时才会参与工作。除非将已有的partition迁移到新的服务器上面。 迁移数据到新broker：迁移数据只能手动执行，但整个过程是自动完成的。Kafka先将新的Server设置为要迁移的partition的follower，让它进行replica。等它将目标partition的数据完全复制，且这个新的server已经进入in sync的列表后，已有的一个replica就会删除自己的partition数据。Kafka提供kafka-reassign-partitions.sh工具做添加新broker后的数据迁移。 kafka-reassign-partitions.sh 使用方法本文记录下笔者的尝试过程，详细的文档说明可以在这里找到：kafka-reassign-partitions.sh依据官方的文档，kafka-reassign-partitions.sh是用来重新分配partition和replica到broker上的工具。简单实现重新分配需要三步： 生成分配计划（generate） 执行分配（execute） 检查分配的状态（verify） 在我原有的集群中，只有一个broker，broker.id = 0；现在我添加了一个broker到集群中，broker.id = 1。下面记录如何对原有的partition进行重新分配。添加broker很简单，只需要在新的机器上拷贝一份新的server.properties，然后改一个新的broker.id，启动即可。 生成分配计划1./bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --topics-to-move-json-file topics-to-move.json --broker-list &quot;1&quot; --generate其中，topic-to-move.json 是事先自己写好的一个json文件，里面包含要进行迁移的topic。例如我用来实验的是这样：1234&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;:&quot;test&quot;&#125;], &quot;version&quot;: 1&#125;用 kafka-topics.sh 工具查看topic信息是：123Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: test Partition: 1 Leader: 0 Replicas: 0 Isr: 0因为原有集群只有一个broker，所以两个partition都在broker 0上，而broker 0本身也是leader，在“in sync”的replica也都是0。执行生成计划的命令以后，会出现类似文档里那样的信息，有两个json格式信息：123456Current partition replica assignment&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0]&#125;,&#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0]&#125;]&#125;Proposed partition reassignment configuration&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1]&#125;,&#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1]&#125;]&#125; 执行分配在生成计划后，可以按照计划来执行重新分配，执行命令：1./bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --execute在官方文档中，提供的例子只有这一行命令与其输入样式。然而，所需的 expand-cluster-reassignment.json 文件里面是什么确没有直接说明。（让我花了好多时间研究=。=）这个文件的内容就是：刚才执行–generate后，第！二！个！json！是的，将它直接粘贴到这个文件里保存就可以了。执行命令后的会出现如下的信息：123456Current partition replica assignment&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0]&#125;,&#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0]&#125;]&#125;Save this to use as the --reassignment-json-file option during rollbackSuccessfully started reassignment of partitions &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1]&#125;,&#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1]&#125;]&#125;此时，Kafka就已经开始进行重新分配、数据迁移了。 检查分配的状态这一步很好操作。只需要执行命令就可以：1./bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --verifyjson文件依然是–execute时的那个。然后会出现如下信息：123Status of partition reassignment:Reassignment of partition [test,1] completed successfullyReassignment of partition [test,0] completed successfully这样就完成了。用kafka-topics.sh工具查看一下当前topic信息就变成了这样：123Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 1 Replicas: 1 Isr: 1 Topic: test Partition: 1 Leader: 1 Replicas: 1 Isr: 1 在这个例子中，我将原来在broker 0上的两个partition都重新分配到了broker 1上。其实可以将两个partition都分在这两个broker上。只需要在生成计划的那一步，指定–broker-list就可以了，将它指定为：–broker-list “0,1”。这个参数就是目标broker的id列表。例如我这样指定后topic的信息就变成了这样：123Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: test Partition: 1 Leader: 1 Replicas: 1 Isr: 1 kafka-topics.sh add partition 添加partition到此我们可以为broker重新分配partition和replica。但是要增加partition怎么办？我问了好久google，google也没有给我合适的答案，倒是告诉了我一个不幸的消息：kafka-add-partition.sh这样的工具只在0.8.0版本中才有。后续的版本都没了，都没了，没了，没…… Orz 那肿么办？！Kafka这样的神器不会傻到不支持增加partition吧？于是就在0.8.2.1提供的工具中查找，发现kafka-topics.sh工具–partitions选项的说明是这样的：123456--partitions &lt;Integer: # of partitions&gt; The number of partitions for the topic being created or altered (WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected看到了吗？“the number of partitions for the topic being created or altered”，然后这个工具有一个选项就是 –alter 啊！试了一下，果然可以！1./bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic test --partitions 4执行后出现如下信息：12WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affectedAdding partitions succeeded!再查看topic信息就变成了这样：12345Topic:test PartitionCount:4 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: test Partition: 1 Leader: 1 Replicas: 1 Isr: 1 Topic: test Partition: 2 Leader: 1 Replicas: 1 Isr: 1 Topic: test Partition: 3 Leader: 0 Replicas: 0 Isr: 0但是注意，就如官方提示所言，在改变partition的数量后，如果消息数据进入partition是以key分配的，那么消息按key分配和消息的顺序都会受影响。这样就添加了partition。 添加partition的这种方法再配合重新分配的方法，就可以当做一种水平扩展的方案。主要就是依赖这两个工具： kafka-topics.sh：添加partition kafka-reassign-partitions.sh：重新分配partition、迁移数据","link":"/2015/10/20/KafkaMirror/"},{"title":"逻辑回归的笔记","text":"之前使用逻辑回归时就接触到里面的sigmod函数，就有疑问为什么会想到用这个函数把线性回归的值压缩到0到1之间；而看到的介绍逻辑回归的公开课、文章，大多数都对此没有详细说明。所以现在又把LR拿出来，记录下从别人文章里学到的解释，同时也理一理逻辑回归的过程和现在的一些理解。 有错误、不当之处欢迎指出！ 线性回归在这之前，需要先提一下线性回归。 线性回归比较简单，从二维的空间比较容易从直观上理解：给定了一组点，然后找到一条线能够拟合这些点。这里所说的“点”就是我们拿到的样本，“线”就是我们要找到的模型（或者说函数）。 线性回归写出来的样子就是： $$h(x) = x_0\\theta_0+x_1\\theta_1+x_2\\theta_2+\\ldots+x_n\\theta_n$$ 用向量的方式表达就是： $$ h(x) = \\theta^TX $$ $\\theta$是我们的参数，$X$是我们的样本。从上面的公式我们能够看到，线性回归的结果的取值范围是实数范围。 它用来预测某事件的数值是没问题的，那么它可不可以用来做分类的判断呢，也就是能不能用于回答“是”或“否”的问题呢？ 从概率的角度看，我们如果能够想办法将这个线性回归的值转变成一个概率值，也就是把它弄到$[0,1]$这个范围内，不是就可以了吗？就变成了通过回答“是的概率有多大”来间接回答“是”或“否”的问题。 利用线性回归的值来回答“是”或“否”在这，我们先抛开“怎么把线性回归的值弄到$[0,1]$之间”这个问题。我们先来看一下我们关心的“是否”问题本身。 还是从概率的角度，我们可以将“是的概率”看作“事件发生的概率”，“回答否的概率”看作“事件不发生的概率”。那么，就可以将“事件发生的概率”设为$P$，而“事件不发生的概率”设为$1-P$。 实际上，我到现在还有一点不太明白大神们是如何想到要做下面这个操作的（我能想到的就是看起来$P$和$1-P$都是操作同一个数值、而$P$又在$[0,1]$之间，所以直觉上会利用它们的比值）： $$ \\frac{P}{1-P} $$ 当$P \\to 0$时，这个的结果是$\\to 0$；当$P \\to 1$时，这个的结果是$\\to \\infty$。所以现在我们再看刚才提到的线性回归的结果范围，我们已经有了一半了：$[0,\\infty]$。 那么剩下的一半呢？我理解，这又是大神们的直觉了，利用了$ln$函数，得到： $$\\ln(\\frac{P}{1-P})$$ 因为$\\frac{P}{1-P}$是在$[0,\\infty]$的，所以正好是$log$函数标准的定义域，然后就得到了其标准的值域：$[-\\infty,\\infty]$，也就刚好是我们线性回归的结果范围了。于是，我们就可以得到： $$\\ln(\\frac{P}{1-P})=\\theta^TX $$$$\\implies \\frac{P}{1-P} = e^{\\theta^TX}$$$$\\implies P = (1-P)e^{\\theta^TX}$$$$\\implies P = \\frac{1}{1+e^{-\\theta^TX}} $$ 然后我们令$z(X) = -\\theta^TX$，就得到了：$$P = \\frac{1}{1+e^z}$$ 这个就是我们常见的sigmod函数了。 现在我们能够利用线性回归的值来回答“事件发生的概率”这个问题了。 那么剩下的问题就是我们如何找到$\\theta$这个参数呢？ 找寻我们的参数$\\theta$现在，我们有了计算“事件发生概率”的计算式，我们也有了一些样本数据。观察这个计算式，我们就差那个$\\theta$了。 那么，“那个$\\theta$是什么”其实直观上一下子能想到这个问题的意思也就是“我们有了一组样本，要怎么计算出那个$\\theta$呢？”。这里，我们需要提一下极大似然（MLE）。 极大似然刚才的问题可以反过来想成是：“什么$\\theta$值能够让我最有可能得到现在这些样本？”。用正式些的说法就是，“找到$\\theta$使出现现有样本可能性最大”。这就是极大似然估计（MLE）的主要思想。用概率中的条件概率表达式写出来就是：$$P(x^{(1)},x^{(2)},x^{(3)},\\ldots,x^{(n)}|\\theta)$$我们这里有一个假设：我们的样本是独立同分布的（也就是说任何两个样本之间的出现没有联系、各自独立出现），并且这些样本能够代表总体样本。那么上面这个等式就可以写成：$$P(x^{(1)}|\\theta)P(x^{(2)}|\\theta)P(x^{(3)}|\\theta) \\ldots P(x^{(n)}|\\theta)$$$$\\implies \\prod_{i=1}^n P(x^{(i)}|\\theta)$$其实是所有样本联合概率分布。 在这里，对于样本$x^{(i)}$，它的概率预测值我们可以写成 $P(x^{(i)})$，我们先不将$\\theta$写出来。由于我们的分类问题是二分类，也就是结果等于1或者0。那么表示出来就是$y^{(i)} = 1$ 或 $y^{(i)} = 0$。那么这个概率预测值可以写成：对于样本$x^{(i)}$，有：$$P(y^{(i)} = 1|x^{(i)})^{y^{(i)}}\\ast P(y^{(i)} = 0|x^{(i)})^{1-y^{(i)}}$$当$y^{(i)} = 1$时，上式等于$P(y^{(i)} = 1|x^{(i)})$；当 $y^{(i)} = 0$时，上式等于$P(y^{(i)} = 0|x^{(i)})$（我们的$\\theta$是在这个$P()$中的，逻辑回归的公式：$P(X)=\\frac{1}{1+e^{-\\theta^{T}X}}$）。 结合极大似然的思想，我们就可以得到：$$\\prod_{i=1}^nP(y^{(i)}=1|x^{(i)})^{y^{(i)}}\\ast P(y^{(i)}=0|x^{(i)})^{1-y^{(i)}}$$ 现在，“极大”就是要求“最大”、求“Max”，也就是求上式的“Max”。这时，我们又可以对其用对数函数来做一步转换。 这里用对数函数做转换的主要原因是，要想求“Max”或是“Min”，得是凸函数（convex）或是凹函数才行。然而上面的式子是非凸的（non-convex），也是非凹的。通过用对数函数转换后的结果是convex的。 转换后的结果是：$$\\log(\\prod_{i=1}^nP(y^{(i)}=1|x^{(i)})^{y^{(i)}}\\ast P(y^{(i)}=0|x^{(i)})^{1-y^{(i)}})$$也就是对这个式子求“Max”。但它是convex的，求“min”要更好一点。就将这个“求Max”的问题转换为求它的对偶问题求“Min”：$min(-\\log())$，而$-\\log()$通过对数函数的性质就可以得到：$$J(\\theta) = \\frac{1}{n}\\ast \\sum_{i=1}^n\\underbrace{y^{(i)}\\log(h(x^{(i)}))+(1-y^{(i)})\\log(1-h(x^{(i)}))}_{h(x^{(i)})=\\frac{1}{1+e^{-\\theta^{T}X^{(i)}}}}$$ 到这里，可以看到这就是逻辑回归的Cost Function了（在这里还没考虑正则化项）。通过对这个式子求解“Min”，得到“min”时的参数就是我们要找寻的参数了。 那么从刚才的过程中可以得知，找到那个“Min”值的时候，我们得到的参数就是我们想要的$\\theta$。那我们如何找到这个“Min”值？ 在这里，这个问题不深入记录。在新的一篇文章里专门来记录几种找到这个“Min”值，这样安排或许更好一点。这里只提一下常用、常听到的一些： 批量梯度下降 随机梯度下降 拟牛顿法 另外还有一个很重要的：正则化。 对逻辑回归的一点感受通过把这整个过程理一遍后，对于逻辑回归的一些特点也有了一些理解。这些之前只是听别人、看文章这么说，但其实理解不够。这下感觉自己对这些特点的理解又深了一些。 在样本数较多时比较好。因为它通过极大似然来推导出cost function，而极大似然的思想中有“现有样本可以代表总体样本”这一点。那么，总体样本当然是很多的，要想代表总体，那自然而然现有样本数是越多越能代表总体了。而且，极大似然是点估计的方法，对参数、模型没有先验在里面，所以样本数越多，知道的信息就越多，就越知道总体样本的“长相”是什么样子。 用于训练的特征之间最好相互联系较少，也就是参数之间尽量独立。这一点自己感觉有一点取决于用什么方法去找“Min”。比如梯度下降中，是对每个参数$\\theta_j$求偏导。那么在对某个$\\theta$求偏导时，其它参数是无差别全部参与进来。个人理解这就相当于切断了与其它参数的联系。如果这个$\\theta$跟其它参数是紧密联系的，这么一切，联系断掉，就有点呵呵了；那如果是没有联系的，就正好符合这过程。 好解释。因为它的公式中$Z(X)=\\theta^TX$本身是一个线性回归。线性回归的思路就容易弄清楚，然后它哪个参数、哪个特征起的作用大也很容易弄清楚。当然，这也需要在指定特征的时候，尽可能使特征、特征值的含义明确。 容易并行化。这跟两个地方有关系，一是逻辑回归预测公式本身，另一个是求“Min”的方法。它本身的公式中$\\theta^TX$，是一个向量，可以并行计算的；另外，还是假设用的梯度下降，梯度下降中求解每个$\\theta_j$偏导的时候，也是可以并行计算的。 参考 机器学习－逻辑回归与最大似然估计 极大似然估计","link":"/2016/12/07/LogisticRegressionNote/"},{"title":"Kafka 基本概念","text":"（Kafka版本 0.8.2.1） Kafka框架下的基本工作模式Kafka是一个消息处理系统。从大面上看，产生消息数据的生产者（Producer）将数据推送到Kafka集群，Kafka集群将消息数据保留起来；然后消费消息数据的消费者（Consumer）向Kafka集群发起请求来消费数据。如图： Kafka基本数据结构消息数据在Kafka中先按照指定的名称进入一组消息队列，这组消息队列有统一的名称。在Kafka中，这样的消息队列组称为Topic。在每个Topic中有多个消息队列partition。每条消息进入Kafka后，按照key被分配到某个partition中，添加到队列的末尾。如下图：对每个partition中的每条数据，kafka会维护一个它在队列中的偏移量（可以理解为类似数组下标的东西）；consumer通过偏移量来读取指定的数据。对于进入Kafka的数据，Kafka会保留一段时间；过了这段时间后，Kafka就会删除最早的一条数据，也就是从队列头的数据。（保留的时间长短可以通过配置文件设置。）在这个过程中，即使数据尚未被消费过也同样会被删除。同时，Kafka也因此会在设置的时间段内保留着所有进入Kafka集群的数据，不论数据是否被消费过。（所以在kafka中的数据可以被重复消费。）（Kafka删除数据的另一种方式是通过每个Partition的大小，partition文件超过设置的大小时就会删除旧数据。）offset与这种数据保留方式的好处有两点： 保证数据占用的空间不会无限制增长； consumer在消费数据时，互相之间是独立的，不会产生影响。 Kafka的分布式结构Kafka的分布式结构体现在物理存储partition上。在物理存储方式上，一个partition被分成了若干份，分别存储在不同的机器上。每个partition的数据也会被复制若干份（数量可配置）。每个partition会有一个“leader”的角色，负责处理所有读写请求；同时也有多个“follower”，当leader挂掉时，其中一个follower会承担起leader的角色，变成新的leader。 ProducerProducer负责产生数据发送到Kafka中。Producer发送数据有两种方式，第一种是随机发送数据到集群中；第二种是以key来指明数据所属的topic与partition。推荐使用第二种方式，因为通过指定key可以将同一类数据发送到同一partition，后端的consumer就可以只针对这个partition来处理数据。Producer采取异步发送数据方式。Producer发送数据时是批量发送，发送的周期可以以积累的数据大小和时间周期两个条件来同时配置。Kafka中的数据可以进行压缩，有gzip和snapy两种格式。关于压缩参考：Kafka数据压缩。关于Producer发送数据的相关配置和API可参考：配置项 和 API 在Producer端，对于数据保证没有做很多工作。Producer可以设置等待响应的时间，也可以设置就一直等到有响应为止。官方对这点的说法是对于数据生产者来说，它只需要做到在数据发送失败时能重发就可以了，对数据的保证（guarantee）的要求并非很强；并且也在考虑将来加入相关的机制。Kafka中Producer的默认发送方式是“at-least-once”（可配置）。 ConsumerConsumer负责从partition中消费数据。每个Consumer被组合成一个Consumer Group。Consumer Group中的每个Consumer可以消费不同partition的数据，但反过来partition只能被一个consumer消费，也因此实际消费的consumer数量不可以超过partition的数量。如下图：这种模式可以同时兼顾到消息处理模拟模型中的queue和publish-subscribe两种方式。另外，在传统的消息处理系统中，由于发送数据与消费数据是异步进行的，所以当消息系统将消息发送出去后，就不能保证数据在consumer一端仍然有序（理解是因为传统的系统中，后端有多个consumer去消费queue中的数据）。而Kafka中从partition到consumer是1对1的关系，所以能保证数据原有的顺序。但是，这种顺序仅在partition中被保证，而无法保证数据在topic层面是有序的。 Consumer读取数据时需要向作为某个partition的leader的broker发送读取该partition数据请求，请求中指明数据的offset和读取的批量大小。因为Consumer本质上是通过指定offset去读取数据的，那么再向Kafka做ack/commit offset是否还有必要？只需要Consumer自己记录下offset就可以进行读取？答案是是的。但这仅限于Consumer永远不会挂掉的情况。否则就需要考虑怎么记录下Consumer的处理位置。 Consumer在请求数据时，采取长连接等待的方式，也就是直到有数据进入时才读取数据；也可以选择设置数据的大小，consumer就会等进入的数据达到设置的大小时才取回数据（这种方式可以确保一次性传输较大的数据，减少网络传输次数等）。从consumer的角度看数据保证（guarantee）的问题。在consumer看来，因为它需要维护读取数据的位置，所以相比Producer而言会复杂一些。官方也对比了三种方式： 读数据 -&gt; 存储位置 -&gt; 处理数据：这种方式下，当在“处理数据”阶段挂掉时，由于已经保存了位置状态，当下次请求数据时就会从下一条数据开始。这样的话，数据就会丢失。这符合“at-most-once”的情况。 读数据 -&gt; 处理数据 -&gt; 存储位置：这种方式下，当consumer在“处理数据”后、“存储位置”前（或“存储位置”的过程中）挂掉时，数据的位置状态没有被记录下来；在下一次的过程中，这条数据就会重复处理一次。这符合“at-least-once”的情况。 读数据 -&gt; 输出数据：这种处理方法与上述两种方法的角度、思路不同。这种方法是将offset存储和输出的数据存储在一起；上述两种方法是基于messaing system中关于数据发送来讨论（“at-most-once”，“at-least-once”，“exactly-once”）。官方的例子是，在他们的Hadoop ETL过程中，将offset和输出数据一起存在HDFS中，这样offset和数据要么一起更新，要么都不更新。","link":"/2015/10/14/KafkaNote/"},{"title":"Kafka Replication","text":"上一篇文章简单记录了Kafka的基本工作框架、数据结构、Producer和Consumer。现在继续啃Kafka的官方文档，主要记录下Kafka中的复制集。（Kafka版本 0.8.2.1） Replication的结构Kafka中通过配置项来指定topic中partition的replication数量。（通常replication面临的问题：通常slave节点的使用率不高，数据间的传输备份会与提升吞吐量造成冲突；同时需要配置的项目也是比较繁琐。）Kafka备份的数据单元是partition，每个partition有一个leader和若干follower。replica的总数取决于所有broker的数量（包括leader）。broker作为工作单元（服务器），一个partition可以有多个broker，反之一个broker下也有多个partition（partition可能是不同topic的）。通常情况下，partition的数量比broker多，而leader也是在这些broker中选出。 同步数据follower同步从leader同步数据的方式与正常的Consumer相同，从leader读取数据然后存入自己的log队列。一条新的数据进入后，在所有“in sync”的replica成功将其放入自己的队列后，这条数据才能被正常的Consumer读取。这个过程在Kafka中被称为“committed”。而“in sync”是指，leader会维护一个满足以下两个条件的replica节点集合： 节点必须能与zookeeper保持正常通信 它必须保证从leader同步的数据不能掉队太多当如果有follower发生异常“dies”、“gets stuck”、“falls behind”时，leader会将其从“in sync”的集合中去掉。（关于“get stuck”的定义配置通过replica.lag.time.max.ms控制，关于“falls behind”的定义配置通过replica.lag.max.messages控制。）","link":"/2015/10/16/KafkaReplication/"},{"title":"Kylin Learning Note —— What? (Kylin, Cube)","text":"What is Kylin?Kylin is an OLAP system developed by the data team of eBay China. Fast is the biggest selling point. It offers second level latency for query over 10 billions rows data. According to the official words, it can let you query big table in Hive at sub-second latency. Sounds cool, right? Another important key point is its idea of processing data, that is pre-processing the big data. What does it mean? Kylin will create some data segments which contain middle level results before we querying. Therefore, Kylin uses extra disk spaces to exchange the low latency performance.Kylin depends on: Hadoop Yarn HBase Hive Zookeeper the versions above are according to the Kylin’s version. It can be found on the doc pages of Kylin. The third point is that Kylin is used for low latency statistic, not searching. If you want to search something like some users’ details, unfortunately, in my opinion, Kylin may not be your best choice. Data Model of KylinThe most important key word is Cube. As you use Kylin, you will create a cube, build a cube, run cube jobs, merge cubes and so on. So, what is a cube? Before answering this question, let’s see how to make a cube ready. To do this, we will do 4 steps: Load Hive tables into Kylin we need Create a data model Design the Cube Build cube I’m not going to talk about the details about these steps. If you want to know more about them, you can find them in my another article. After we loading the hive tables into Kylin, we need to create a data model first. A data model is a description about what hive tables, dimensions (such as columns used for group by), measures (such as columns used for statistic) we need. It is not a real data block.And then we need to create a cube description. It contains what dimensions we need, same as the model; and what measures we need, the difference from the model is that we design which statistic function we use here; and the time range. It is also not a data block yet, but it is called cube_desc and stored in HBase.At last we build a cube according to the information created above two. At this step, it creates the real data segments on HDFS stored in HBase. So, in a view, we can say that the model and the cube are logical definitions and the data segment is a physical definition. A cube is consists of numbers of segments. References Kylin Home Page OLAP Wikipedia","link":"/2016/03/23/KylinIntroduction/"},{"title":"Java中Synchronized、Volatile、Atomic的一些事","text":"Summary of synchronized、atomic、volatile in Java在学习Java并发的过程中必然会遇到同步的问题；而且有时会在synchronized、volatile和atomic之间傻傻分不清楚。由此，专门去学习了一下这三者。本文就算是学习笔记。java版本1.8 线程安全 Thread Safe describe some code that can be called from multiple threads without corrupting the state of the object or simply doing the thing the code must do in right order. —— 《Java Concurrency in Practice》, Brian Goetz 直译过来的意思就是，当一段代码被多个线程调用时，其中的被使用的对象的状态不发生冲突，或按照正确的顺序执行任务。 一个比较简单的例子展示不是线程安全：多线程下执行i++的操作， // 1. 先取得 i 的值 // 2. 执行 +1 tmp = i + 1; // 3. 赋值 i = tmp;如果是多线程的情况下，其实无法确定每个线程会在什么时候执行到上述的三个步骤，也就会导致每个线程获取到的i值不同，或是输出结果并不是我们想要的。 那么，如何做到线程安全呢？ 目前能想到的思路有： 使用锁，synchronized 关键字 使用锁，Lock类 利用原子操作，atomic 利用Java提供的volatile 接下来，会就这三者写下一些记录。 锁，synchronized在java中，任何对象都可以是锁，或者说，任何对象都可以被锁住。 对于同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前对象的Class对象。 对于同步方法块，锁是Synchonized括号里配置的对象。 —— 《聊聊并发（二）—— Java SE1.6中的Synchronized》by 方腾飞 java的每个对象都有一个monitor对象，monitor对象有“进入数”这么一个值。 synchronized关键字的适用范围： 修饰代码块 修饰方法 下面就这两者分别记录一下： 修饰代码块 synchronized里，这个monitor有两条指令：monitorentrer 和 monitorexit，它们是成对出现的。在JVM的规范中，对这两条指令是这么描述的： 对于monitorenter： Each object is associated with a monitor. A monitor is locked if and only if it has an owner. The thread that executes monitorenter attempts to gain ownership of the monitor associated with objectref, as follows:• If the entry count of the monitor associated with objectref is zero, the thread enters the monitor and sets its entry count to one. The thread is then the owner of the monitor.• If the thread already owns the monitor associated with objectref, it reenters the monitor, incrementing its entry count.• If another thread already owns the monitor associated with objectref, the thread blocks until the monitor’s entry count is zero, then tries again to gain ownership. 简单说，monitorenter指令是指线程尝试竞争获取monitor的所有权。当monitor的进入数为0时，线程进入monitor然后将其设置为1；如果线程已经占有这个monitor，就将进入数+1；如果有另一个线程尝试竞争monitor，就得一直阻塞着，直到进入数为0，再去竞争获取monitor所有权。 对于monitorexit： The thread that executes monitorexit must be the owner of the monitor associated with the instance referenced by objectref. The thread decrements the entry count of the monitor associated with objectref. If as a result the value of the entry count is zero, the thread exits the monitor and is no longer its owner. Other threads that are blocking to enter the monitor are allowed to attempt to do so. 即是，将monitor的进入数-1，且只有拥有这个monitor的线程才能执行这个操作。 修饰方法 synchronized修饰方法时，在字节码中这个方法会有一个ACC_SYNCHRONIZED标识。JVM会隐式执行enter和exit，不会有显式的monitorenter和monitorexit指令。实际上其过程与修饰代码块时是一样的。 好了，synchronized 用于锁一个对象的过程如上。但在上面的描述中有一个问题：线程是怎么竞争monitor的呢？ 要回答这个问题，得先了解一下CAS操作和synchronized对应的3种有锁的状态： 偏向锁状态 轻量级锁状态 重量级锁状态 先来简单提一下CAS操作。 CAS CAS全称是 Compare and Swap，比较并设置。这是在硬件层面上做的原子操作：比较目标值，如果与拿到的当前值一致，则修改为新值；不一致，则不修改。 In computer science, compare-and-swap (CAS) is an atomic instruction used in multithreading to achieve synchronization. It compares the contents of a memory location to a given value and, only if they are the same, modifies the contents of that memory location to a given new value. This is done as a single atomic operation. —— Wikipedia 刚才提到的设置进入数这样的操作，即是通过CAS来完成。 上述那3种状态是存储在java对象的对象头里，有具体的状态值表示这些状态，引入这些状态是为了减少获得锁和释放锁的性能消耗。这3种状态之间的转换关系是：偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁，随着竞争情况逐渐升级，但不能降级。 偏向锁状态偏向锁中的“偏向”的含义其实是说“指向占用锁的线程”。在java对象头里，还有个值记录占有锁的线程ID号。 加锁： 当有一个线程获取锁后，会将这个值设置成自己的线程ID号。然后有线程来尝试获取这个锁时，先比较这个线程ID号。如果与当前线程一致，则直接获取锁然后执行同步代码块，不需要执行CAS操作来加锁。如果不一致，则看一下偏向锁的标识是否设置成1，如果设置了则用CAS来尝试将线程ID指向自己；如果没有设置，则用CAS来竞争锁。 释放： 持有锁的线程会等到有其它线程来竞争锁时才会释放锁。撤销时，会暂停持有锁的线程，然后将线程ID设置为空，然后唤醒被暂停的线程。 其实在“设置线程ID为空”这一步时，还有细节的操作： 偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态，如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。 —— 《聊聊并发（二）——Java SE1.6中的Synchronized》by 方腾飞 为什么偏向锁是这么设置的呢？Hotspot的作者经过以往的研究发现大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。 轻量级锁JVM会创建用于存储锁记录的空间，并将对象头里锁的状态复制到这个空间里。（创建的地方是线程的栈帧中；这个过程参考官方的Displaced Mark Word。） 加锁： 线程用CAS操作尝试将对象头中的锁状态替换为指向刚才提到的锁记录的指针。如果成功，则线程成功获得锁；如果失败，线程则用自旋来尝试获取锁。 释放： 解锁过程即是线程用CAS操作来将锁记录中的值替换回对象头中。 在这个释放过程中，如果成功了，则说明没有竞争。如果失败了，则说明有竞争，锁状态则会升级成为重量级锁。这时当有其它线程尝试获取锁时，就会blocking。 重量级锁其实即是开始时提到的enter、exit的基本过程：竞争锁、blocking的方式。 上述内容是目前我对synchronized的理解。 Volatile在记录Atomic前，需要先了解一下volatile关键字。volatile关键字在java中是用于修饰变量的。 多核的处理器每个核都有自己的cache；对于普通的变量来说，cpu读取并修改这个变量值的过程是： 从main memory中读取该变量的值，然后放在自己的缓存中； 下次读的时候先从缓存中找 在写这个值的时候，cpu也是先将值写到缓存中，然后才会更新到内存。 在这样的过程中，多个线程同时从内存中取出某个变量值后，再更新、再读，这个过程在线程之间是不可见的，很可能造成内存中该变量的值错误。 而volatile关键字的作用，就会让这个变量不会从缓存中取出，更新时也不会更新到缓存，而是直接读写内存。这样，每个线程去读写这个值的时候，都能保证对其它线程来说是可见的。 当把变量声明为volatile类型后，编译器与运行时都会注意到这个变量是共享的，因此不会将该变量上的操作与其它内存操作仪器重排序。volatile变量不会缓存在寄存器或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。 ——《Java Concurrency in Practice》 volatile相比synchronized而言，是一个轻量级的同步机制。但它的用法跟synchronized不同。volatile只能保证可见性，不能保证原子性，而加锁可以同时保证这两者；相对地，使用volatile的开销也比加锁要轻许多。因此，当使用volatile变量时，应注意需要满足以下几点： 对变量的写入操作不依赖变量当前的值，或者只有一个线程能对其写。 该变量不会与其它状态变量一起纳入不变性条件中。 在访问变量时不需要加锁。 Atomic，原子操作参考资料 聊聊并发（二）——Java SE1.6中的Synchronized Java并发编程：Synchronized及其实现原理 比较并交换(compare and swap, CAS)","link":"/2016/12/04/SomethingAboutThreadSafeInJava/"},{"title":"Spark学习笔记记录点","text":"Overview 有两种对象： RDD（Resillient Distributed Dataset），弹性分布式数据集，可以存储任何形式的数据； 共享变量（Shared Variables），可以在不同的集群任务中共享，提供两种共享变量： boardcast variable：在不同任务之间共享的变量，只读 accumulators：在不同任务之间共享的计数器，可用于求和 连接Spark集群 Java 12SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);JavaSparkContext sc = new JavaSparkContext(conf); 第一行是创建Spark的集群信息对象，第二行是连接集群；任何一个Spark的应用都通过一个Context对象来连接集群： The first thing a Spark program must do is to create a JavaSparkContext object, which tells Spark how to access a cluster. To create a SparkContext you first need to build a SparkConf object that contains information about your application. RDD(Resillient Distributed Dataset)RDD是一个可容错、可并行处理的数据集合。 创建RDD： 有两种方式来创建RDD：创建并行处理集合 和 使用外部数据。 并行数据集合（Parallelized Collections）：由我们自己的程序driver program生成，Java中通过JavaSparkContext对象的parallelize方法指定： 12List&lt;Integer&gt; data = Arrays.asList(1, 2, 3, 4, 5);JavaRDD&lt;Integer&gt; distData = sc.parallelize(data);parallelize方法可以传入分区参数，告诉Spark将输入数据分成多少个分区。 外部数据集（External Datasets）：任务Hadoop格式的数据，如HDFS、HBase，甚至可以是AWS S3。通过SparkContext对象的textFile方法指定： 1JavaRDD&lt;String&gt; distFile = sc.textFile(\"data.txt\");外部数据集可以是一般的TextFiles、SequenceFile，或是Hadoop支持的其它任何格式的数据。外部数据集的输入可以直接指定目录、包含通配符的文件名，也支持直接指定gz文件。 外部数据集同样支持指定分区，默认是一个Hadoop的block对应一个分区。 详细外部数据集说明信息参见（主要是note部分）：External Datasets* RDD操作：transformations 和 actions transformations: 从一个已有数据集创建出一个新的数据集（理解为原数据集的变换） create a new dataset from an existing one transformations的操作在创建时并不会马上执行，只有在调用actions的方法时才会开始执行。 actions: 对数据集做处理，例如统计计数等，返回一个值 return a value to the driver program after running a computation on the dataset 每个经过transform的RDD都可以重复使用：通过 persist 方法。 官方代码示例： 12345678910JavaRDD&lt;String&gt; lines = sc.textFile(\"data.txt\"); //use an external dataset// laziness, it will bot be computed hereJavaRDD&lt;Integer&gt; lineLengths = lines.map(s -&gt; s.length());// cache the transformed lineLengths RDD, declear before use reduce methodlineLengths.persist();// actionsint totalLength = lineLengths.reduce((a, b) -&gt; a + b); transformation 的方法：Functions of Transformations actions 的方法：Functions of Actions shuffle operationsshuffle operations是一种很消耗资源的操作，官方并不推荐使用这种方式； Java代码使用方式参考：Passing Functions to Spark","link":"/2015/09/08/SparkLearningNote/"},{"title":"Stream Processing学习笔记","text":"最近在学习实时流式数据处理的相关知识。这篇文章就作为学习笔记来整理、记录，所谓“好记性不如烂笔头”。 学习的资料不是某本书，也不是某某教程，对于这样概念性和实践性都比较强、同时又主要是依赖于一些组件工具的东西，只需要在网络上搜索相关的技术文档就好了。因此，我学习的材料是Samza的官方文档。Samza是一个很年轻的组件，依赖Kafka。我并非要使用它，而是它的官方文档不仅写了使用Samza的相关信息，还写了关于Stream Processing的概念。 Stream Processing概念Stream Processing依赖于消息系统（Messaging System）。先简单说下什么是messaging system： A messaging system is a fairly low-level piece of infrastructure—it stores messages and waits for consumers to consume them. – Samza 消息系统即是一个将消息数据缓存在消息队列中，等待消息处理程序去取数据的系统。 而Stream Processing是建立在消息系统之上的一个数据处理概念： Stream processing is a higher level of abstraction on top of messaging systems, and it’s meant to address precisely this category of problems. – Samza Stream Processing建立在消息系统之上，可能一个消息系统也可能多个；同时，在Stream Processing过程中包括如何处理消息系统中若干问题。 Stream Processing的常见问题由于Stream Processing是建立在消息系统这样的基础设施之上的，所以消息系统本身的问题同时也是Stream Processing的问题。 如果数据的消费程序挂掉、当前数据丢掉了，怎么处理？丢掉的数据如何恢复？ 重启的程序应该从什么位置继续工作？ 如果依赖的消息系统出现数据重发、数据漏发，怎么处理？ 如何对消息数据进行分组处理？ 消息数据处理进程的状态如何维护？ SamzaSamza的Job、partition和task下面的笔记关于Samza的工作模式，其中的方式可以借鉴到一般的Stream Processing中。Samza的一个job按数据管理方式会被分成若干个partition，按工作进程会被分成若干个task。 数据以Kafka 的方式按Key被分到不同的partition中，相同key的数据一定会被分到同一个partition中。 Task是job中具体执行数据处理的工作单元，一个task不局限于某一个job，它可以接收多个输入流，也就是说一个task可以从多个partition上读取数据；但反过来，一个partition的数据只能由一个task读取（从某一个task角度看，是1对N的关系；从某一个partition角度看，是1对1的关系。），因此partition的数量不能超过task的数量（有可能会出现task没有对应partition的情况）。多个task可以运行在多台机器上，其中资源的调度是通过yarn分配的。 在Samza中的数据流与job之间可以形成一个数据流图（Dataflow Graphs），如图：这样的好处是不同的job不必都写在同一份代码中；同时在这种方式下，对下游job进行操作时不影响上游job。 The jobs are otherwise totally decoupled: they need not be implemented in the same code base, and adding, removing, or restarting a downstream job will not impact an upstream job. – Samza","link":"/2015/10/12/StreamLearning/"},{"title":"流数据平台阅读笔记","text":"原文：Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform 这篇文章是Kafka开发团队的Leader Jay Kreps所写。文章里主要介绍一个流式数据处理平台的概要，也包括这位大牛在实际工作中的经验。 核心理念这篇文章分为Part1和Part2，分别介绍了两点：第一，以流数据为核心的数据处理平台的架构，任何数据都可以是流式的数据；第二，在数据平台中，统一数据格式，并且使用一种带有schema信息的数据格式。 一切皆为流这个说法也许有些偏激。但笔者同样赞同作者的这个观念，即进入数据系统的任何数据都可以看作是流式数据。外部业务上例如电商平台，用户的点击、搜索、购买行为数据都是一个接一个的Event Flow；内部系统上如系统产生的日志，也是随时间推移产生的日志流，甚至是关系型数据库的操作也是一连串的Event Flow（如MySQL的binlog）。 在这个概念下，我们就可以想象有这么一个大管子，它的里面细分很多小管子，每个小馆子里面都在源源不断地流淌着某种类型的数据流。然后在这个大管子上有很多的槽口，可以跟管子外的其它部件对接。这些部件把数据流从管子里取出来处理；处理后的结果可以再放回大管子里的某个不同于之前的小管子里，或者不放回去。这些连接到管子上的部件可以是Applications、Hadoop、Monitor、RDS、Analysis Server等等。就有了下面这张图：各个部件在接入管子时也有上下游之分，所以在数据的处理流程上也许是这样的： 统一“度量衡”统一数据格式，统一“度量衡”。战国时期各国之间度量衡各不相同，造成各国之间要相互转化标准十分麻烦，使文化、经济难以相通；Unix系统下，因为有管道“|”这个神器，就可以让各种命令之间的结果以统一的ASCII Text格式在标准输出与标准出入之间无障碍传递，极大提高Unix系统的易用性。 这一反一正两个例子充分说明作者提出的“统一数据格式”的观念是多么重要。并且，不仅要统一数据格式，还要使用一种带有schema信息的数据格式。这样的好处是： 统一数据格式使数据平台里各个组件、服务之间的数据交流变得很简单，易维护；省去了转换数据格式的工作量，同时也避免由此带来的数据异常问题。 使用带有schema信息的数据格式，会让数据使用变得灵活、简洁。schema信息相当于是数据的含义描述，当下游有很多使用者时，下游的人可以依据描述自行决定数据使用而不依赖、影响别的服务；同时上下游的生产者与消费者也极大减少了不必要的交流成本。（详细关于schema的说明可以参考原文。） 由上可给数据平台带来更好的扩展性。 对于数据格式的选择，作者推荐Avro，理由可参考原文。","link":"/2015/12/15/StreamDataCentral/"},{"title":"2015 我的总结","text":"2015年对自己来说应该是一个转折点，人生中一个重要的转折点。在这一年里，我做出了很多选择，有得有失。记得在2015年初始的时候对自己定了一些目标，现在回顾一下，很高兴，比较重要的都实现了。 回首这一年，从年初到年末，仿佛像看电影一般，一帧帧的画面在脑海里闪现。突然感到自己这一年还是做了挺多事情，不算浑浑噩噩、不算浪费。 2015年里，我很高兴我能坚持了自己一年至少出行两次的计划：5月初的康定新都桥，以及10月的马来西亚。年初时本计划去泰国的，后来怎么变了，其实原因已经不记得了。","link":"/2016/02/04/Summary2015/"}],"tags":[{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"Messaging System","slug":"Messaging-System","link":"/tags/Messaging-System/"},{"name":"Config","slug":"Config","link":"/tags/Config/"},{"name":"Flink","slug":"Flink","link":"/tags/Flink/"},{"name":"Window","slug":"Window","link":"/tags/Window/"},{"name":"流计算","slug":"流计算","link":"/tags/%E6%B5%81%E8%AE%A1%E7%AE%97/"},{"name":"Logistic Regression","slug":"Logistic-Regression","link":"/tags/Logistic-Regression/"},{"name":"Linear Regression","slug":"Linear-Regression","link":"/tags/Linear-Regression/"},{"name":"MLE","slug":"MLE","link":"/tags/MLE/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"极大似然估计","slug":"极大似然估计","link":"/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"},{"name":"Kylin","slug":"Kylin","link":"/tags/Kylin/"},{"name":"OLAP","slug":"OLAP","link":"/tags/OLAP/"},{"name":"Cube","slug":"Cube","link":"/tags/Cube/"},{"name":"Data Model","slug":"Data-Model","link":"/tags/Data-Model/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"synchronized","slug":"synchronized","link":"/tags/synchronized/"},{"name":"volatile","slug":"volatile","link":"/tags/volatile/"},{"name":"atomic","slug":"atomic","link":"/tags/atomic/"},{"name":"thread safe","slug":"thread-safe","link":"/tags/thread-safe/"},{"name":"concurrency","slug":"concurrency","link":"/tags/concurrency/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Distributed System","slug":"Distributed-System","link":"/tags/Distributed-System/"},{"name":"StreamProcessing","slug":"StreamProcessing","link":"/tags/StreamProcessing/"},{"name":"Streaming System","slug":"Streaming-System","link":"/tags/Streaming-System/"},{"name":"Data","slug":"Data","link":"/tags/Data/"},{"name":"Data Central","slug":"Data-Central","link":"/tags/Data-Central/"},{"name":"Summary","slug":"Summary","link":"/tags/Summary/"},{"name":"2015","slug":"2015","link":"/tags/2015/"}],"categories":[{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"},{"name":"逻辑回归","slug":"技术/逻辑回归","link":"/categories/%E6%8A%80%E6%9C%AF/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"Kafka","slug":"技术/Kafka","link":"/categories/%E6%8A%80%E6%9C%AF/Kafka/"},{"name":"流计算","slug":"技术/流计算","link":"/categories/%E6%8A%80%E6%9C%AF/%E6%B5%81%E8%AE%A1%E7%AE%97/"},{"name":"Kylin","slug":"技术/Kylin","link":"/categories/%E6%8A%80%E6%9C%AF/Kylin/"},{"name":"Concurrency","slug":"技术/Concurrency","link":"/categories/%E6%8A%80%E6%9C%AF/Concurrency/"},{"name":"Spark","slug":"技术/Spark","link":"/categories/%E6%8A%80%E6%9C%AF/Spark/"},{"name":"生活","slug":"生活","link":"/categories/%E7%94%9F%E6%B4%BB/"}]}